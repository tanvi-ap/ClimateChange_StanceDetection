{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ISRP_LSTM.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Stance Detection </h1> \n",
        "Team Members: Tanvi Padhye [Z1906477],Ankita Ratnaparkhi [Z1907718]"
      ],
      "metadata": {
        "id": "-ZwXLM8ovyKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import the necessary files** <br>\n",
        "This step is to import all the necessary files."
      ],
      "metadata": {
        "id": "W8EWiZajv_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import operator\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvIV0BYLUdCX",
        "outputId": "31b39915-956f-4c6f-9a67-0013f2308800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Loading the data set**  <br>\n",
        "The dataset used for this project is EDA climate change dataset from Kaggle which contains 15000 tweets. <br>"
      ],
      "metadata": {
        "id": "6Qjl0brwwHA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f8HgercoTz1A",
        "outputId": "5402c37f-a438-44b8-d7ff-631e8688bfd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stance</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stance                                            message  tweetid\n",
              "0       1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1       1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2       2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3       1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4       1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "path = \"https://raw.githubusercontent.com/iAnkitar/ISR_Spotlight/master/train.csv\"\n",
        "r = requests.get(path)\n",
        "train_df = pd.read_csv(io.StringIO(r.text),header=0)\n",
        "\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the news from the database\n",
        "train_df = train_df.drop(train_df[train_df.stance == 2].index)"
      ],
      "metadata": {
        "id": "iYWD6WHpVmE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing the number of tweets according to the stance "
      ],
      "metadata": {
        "id": "KDnilwokwZ-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(train_df.stance.unique())\n",
        "print(train_df['stance'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAJh_ojXWS6k",
        "outputId": "13e3c29e-6f14-4874-f815-855cc6580e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12179\n",
            "[ 1  0 -1]\n",
            " 1    8530\n",
            " 0    2353\n",
            "-1    1296\n",
            "Name: stance, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocessing**<br>\n",
        "The dataset needs to be processed before any other steps. We will achieve that by removing the stop words, punctuations, cleaning the text and any removal of any special characters.<br><br>**Simple text cleaning processes**: Some of the common text cleaning process involves: <br>\n",
        "\n",
        "\n",
        "1.  Removing leading,trailing & extra white spaces/tabs\n",
        "2.  Removing punctuations, special characters, URLs & hashtags\n",
        "3.  Correcting any typos and slangs and abbreviations.\n",
        "\n",
        "**Stop-word removal:** Using nltk a generic list of stop words such as 'i','you','a','the' can be removed. <br> <br>"
      ],
      "metadata": {
        "id": "KDskKMaTyPL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "train_df['message'] = train_df['message'].apply(clean_text)\n",
        "train_df['message'] = train_df['message'].str.replace('\\d+', '')"
      ],
      "metadata": {
        "id": "zuNIW_nhZ6NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each tweet.\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(train_df['message'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4qhGmy4gXK4",
        "outputId": "7c5c6f0c-5544-465f-9a00-58cdadcf78b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26147 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X = tokenizer.texts_to_sequences(train_df['message'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVmfOIUVhTPU",
        "outputId": "666d06cc-ab3e-4864-d76b-fbb033bce685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (12179, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(train_df['stance']).values\n",
        "\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9bVBAixhcmD",
        "outputId": "2a3f34b9-abf7-49fb-9362-cd0ee3e2525f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (12179, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the dataset into train(80%), test(10%) and validation(10%)."
      ],
      "metadata": {
        "id": "GPb7M4pVxYfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYUA3HkJhpuo",
        "outputId": "2b10451b-28bf-4f56-a52c-0c63fda6ae2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9743, 50) (9743, 3)\n",
            "(2436, 50) (2436, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code is used for Creating the model that we will use for stance detection"
      ],
      "metadata": {
        "id": "LNRAAZLyxrIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u11Nv8LUhvZG",
        "outputId": "12660e33-76cd-4e03-8c50-94ac18570969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 50, 100)           5000000   \n",
            "                                                                 \n",
            " spatial_dropout1d_1 (Spatia  (None, 50, 100)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,080,703\n",
            "Trainable params: 5,080,703\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "69/69 [==============================] - 14s 167ms/step - loss: 0.7823 - accuracy: 0.7035 - val_loss: 0.6748 - val_accuracy: 0.7262\n",
            "Epoch 2/5\n",
            "69/69 [==============================] - 11s 161ms/step - loss: 0.5182 - accuracy: 0.7907 - val_loss: 0.6088 - val_accuracy: 0.7590\n",
            "Epoch 3/5\n",
            "69/69 [==============================] - 11s 161ms/step - loss: 0.3188 - accuracy: 0.8743 - val_loss: 0.6352 - val_accuracy: 0.7282\n",
            "Epoch 4/5\n",
            "69/69 [==============================] - 11s 162ms/step - loss: 0.1645 - accuracy: 0.9404 - val_loss: 0.7355 - val_accuracy: 0.7385\n",
            "Epoch 5/5\n",
            "69/69 [==============================] - 11s 160ms/step - loss: 0.0742 - accuracy: 0.9757 - val_loss: 0.8388 - val_accuracy: 0.7641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model for test data and printing the accuracy."
      ],
      "metadata": {
        "id": "tRllP5_Fx05J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhicZrk9iK5H",
        "outputId": "bd7d80d9-4343-4196-a5db-34bde602d6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77/77 [==============================] - 1s 13ms/step - loss: 0.8207 - accuracy: 0.7697\n",
            "Test set\n",
            "  Loss: 0.821\n",
            "  Accuracy: 0.770\n"
          ]
        }
      ]
    }
  ]
}